# æ”¹é€²è·¯ç·šåœ– - YOLO11 æ¨ç†ç³»çµ±

**ç‰ˆæœ¬**: v1.0  
**å‰µå»ºæ—¥æœŸ**: 2026-01-06  
**ç›®æ¨™**: å¾ç”Ÿç”¢å¯ç”¨é‚å‘ä¼æ¥­ç´šç³»çµ±

---

## ğŸ¯ æ•´é«”ç›®æ¨™

å°‡å°ˆæ¡ˆå¾ç•¶å‰ **7.5/10** æå‡è‡³ **9.0/10**ï¼Œé‡é»æ”¹é€²ï¼š
- å¯é æ€§ï¼ˆReliabilityï¼‰
- å¯ç¶­è­·æ€§ï¼ˆMaintainabilityï¼‰
- å¯è§€æ¸¬æ€§ï¼ˆObservabilityï¼‰
- å®‰å…¨æ€§ï¼ˆSecurityï¼‰

---

## ğŸ“… Phase 1: ç·Šæ€¥ä¿®å¾©ï¼ˆç¬¬ 1 é€±ï¼‰

### ç›®æ¨™ï¼šä¿®å¾©é—œéµé¢¨éšªï¼Œç¢ºä¿ CI ç©©å®š

#### Task 1.1: ä¿®å¾©ä¾è³´ç®¡ç† âš¡ P0
**å•é¡Œ**: `requirements.txt` åªæœ‰ `-e .`ï¼Œå¤–éƒ¨ç’°å¢ƒç„¡æ³•å®‰è£

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. å®‰è£ pip-tools
pip install pip-tools

# 2. ç”Ÿæˆå®Œæ•´ä¾è³´
pip-compile pyproject.toml -o requirements.txt

# 3. ç”Ÿæˆé–‹ç™¼ä¾è³´
pip-compile pyproject.toml --extra dev --extra gui -o requirements-dev-full.txt
```

**é©—è­‰**:
```bash
# åœ¨æ–°ç’°å¢ƒæ¸¬è©¦
python -m venv test_env
source test_env/bin/activate  # Windows: test_env\Scripts\activate
pip install -r requirements.txt
pytest -v
```

**å·¥æ™‚**: 2 å°æ™‚  
**è² è²¬äºº**: DevOps/é–‹ç™¼åœ˜éšŠ

---

#### Task 1.2: ç§»é™¤ CI continue-on-error âš¡ P0
**å•é¡Œ**: CI ä¸­å¤šè™•ä½¿ç”¨ `continue-on-error: true`ï¼Œéš±è—çœŸå¯¦å•é¡Œ

**è§£æ±ºæ–¹æ¡ˆ**:

```yaml
# .github/workflows/ci.yml - ä¿®æ”¹å‰
- name: Lint with ruff
  run: ruff check .
  continue-on-error: true  # âŒ ç§»é™¤

# ä¿®æ”¹å¾Œ
- name: Lint with ruff
  run: ruff check .
  # å¦‚æœæœ‰å·²çŸ¥å•é¡Œï¼Œä½¿ç”¨ ignore æˆ– baseline

- name: Run tests (Ubuntu)
  run: pytest -v -m "not gui"
  # âŒ ç§»é™¤ continue-on-error
```

**ä¿®å¾©æ­¥é©Ÿ**:
1. åŸ·è¡Œæœ¬åœ°æ¸¬è©¦æ‰¾å‡ºå¤±æ•—åŸå› 
2. ä¿®å¾©å¤±æ•—çš„æ¸¬è©¦æˆ–æ¨™è¨˜ç‚º `xfail`
3. ç§»é™¤æ‰€æœ‰ `continue-on-error`

**å·¥æ™‚**: 4 å°æ™‚  
**è² è²¬äºº**: QA/é–‹ç™¼åœ˜éšŠ

---

#### Task 1.3: åŠ å…¥è·¯å¾‘å®‰å…¨é©—è­‰ ğŸ” P0

**æ–°å¢æ–‡ä»¶**: `core/security.py`

```python
"""Security utilities for safe file operations."""
from pathlib import Path
from typing import Union

class SecurityError(Exception):
    """Raised when a security check fails."""
    pass

class PathValidator:
    """Validate file paths to prevent directory traversal attacks."""
    
    def __init__(self, allowed_roots: list[Path]):
        self.allowed_roots = [Path(root).resolve() for root in allowed_roots]
    
    def validate_path(self, path: Union[str, Path], *, must_exist: bool = False) -> Path:
        """Validate that a path is safe to access.
        
        Args:
            path: Path to validate
            must_exist: If True, verify path exists
            
        Returns:
            Resolved absolute path
            
        Raises:
            SecurityError: If path is outside allowed roots
            FileNotFoundError: If must_exist=True and path doesn't exist
        """
        resolved = Path(path).resolve()
        
        # Check if path is within allowed roots
        if not any(self._is_relative_to(resolved, root) for root in self.allowed_roots):
            raise SecurityError(
                f"Access denied: {path} is outside allowed directories"
            )
        
        if must_exist and not resolved.exists():
            raise FileNotFoundError(f"Path does not exist: {path}")
        
        return resolved
    
    @staticmethod
    def _is_relative_to(path: Path, parent: Path) -> bool:
        """Check if path is relative to parent (Python 3.9+ has this built-in)."""
        try:
            path.relative_to(parent)
            return True
        except ValueError:
            return False

# å…¨å±€å¯¦ä¾‹
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
path_validator = PathValidator(allowed_roots=[
    PROJECT_ROOT,
    PROJECT_ROOT / "models",
    PROJECT_ROOT / "Result",
])
```

**ä¿®æ”¹ `core/config.py`**:
```python
from core.security import path_validator

class DetectionConfig:
    def load_config(self, config_path: str | Path):
        # é©—è­‰è·¯å¾‘å®‰å…¨
        safe_path = path_validator.validate_path(config_path, must_exist=True)
        with open(safe_path) as f:
            ...
```

**å·¥æ™‚**: 6 å°æ™‚ï¼ˆåŒ…å«æ¸¬è©¦ï¼‰  
**è² è²¬äºº**: å®‰å…¨/é–‹ç™¼åœ˜éšŠ

---

#### Task 1.4: YAML å®‰å…¨è¼‰å…¥ ğŸ” P0

**æœå°‹ä¸¦æ›¿æ›æ‰€æœ‰ `yaml.load()`**:
```bash
# æ‰¾å‡ºæ‰€æœ‰ä½¿ç”¨ yaml.load çš„åœ°æ–¹
grep -r "yaml.load" --include="*.py" .
```

**æ›¿æ›ç‚º**:
```python
# âŒ å±éšª
config = yaml.load(f)

# âœ… å®‰å…¨
config = yaml.safe_load(f)
```

**å·¥æ™‚**: 2 å°æ™‚  
**è² è²¬äºº**: å®‰å…¨/é–‹ç™¼åœ˜éšŠ

---

### Phase 1 ç¸½çµ
- **ç¸½å·¥æ™‚**: 14 å°æ™‚ï¼ˆ2 å·¥ä½œæ—¥ï¼‰
- **äº¤ä»˜æˆæœ**: CI ç©©å®šé€šéã€ä¾è³´å¯å¾©ç¾ã€åŸºç¤å®‰å…¨åŠ å›º
- **é©—æ”¶æ¨™æº–**: 
  - âœ… CI å…¨ç¶ 
  - âœ… `pip install -r requirements.txt` åœ¨æ–°ç’°å¢ƒå¯ç”¨
  - âœ… è·¯å¾‘æ³¨å…¥æ¸¬è©¦é€šé

---

## ğŸ“… Phase 2: å¯è§€æ¸¬æ€§å¢å¼·ï¼ˆç¬¬ 2-4 é€±ï¼‰

### ç›®æ¨™ï¼šå¢åŠ æ€§èƒ½ç›£æ§ã€æ—¥èªŒå¢å¼·ã€å‘Šè­¦ç³»çµ±

#### Task 2.1: æ•´åˆ Prometheus æŒ‡æ¨™ ğŸ“Š

**å®‰è£ä¾è³´**:
```bash
pip install prometheus-client
```

**æ–°å¢æ–‡ä»¶**: `core/monitoring.py`

```python
"""Prometheus metrics for monitoring."""
from prometheus_client import Counter, Histogram, Gauge, Info
import time

# æ¨ç†å»¶é²ç›´æ–¹åœ–
INFERENCE_LATENCY = Histogram(
    'inference_latency_seconds',
    'Inference latency in seconds',
    ['product', 'area', 'inference_type', 'status'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

# æ¨ç†è¨ˆæ•¸å™¨
INFERENCE_COUNT = Counter(
    'inference_total',
    'Total number of inferences',
    ['product', 'area', 'inference_type', 'status']
)

# æ¨¡å‹è¼‰å…¥è¨ˆæ•¸
MODEL_LOAD_COUNT = Counter(
    'model_load_total',
    'Total number of model loads',
    ['product', 'area', 'inference_type']
)

# ç•¶å‰è¼‰å…¥çš„æ¨¡å‹æ•¸
LOADED_MODELS = Gauge(
    'loaded_models_count',
    'Number of currently loaded models'
)

# æª¢æ¸¬çµæœçµ±è¨ˆ
DETECTION_RESULTS = Counter(
    'detection_results_total',
    'Detection results by status',
    ['product', 'area', 'result_status']  # PASS/FAIL
)

# GPU è¨˜æ†¶é«”ä½¿ç”¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
GPU_MEMORY_USAGE = Gauge(
    'gpu_memory_usage_bytes',
    'GPU memory usage in bytes',
    ['device']
)

class InferenceTimer:
    """Context manager for timing inference operations."""
    
    def __init__(self, product: str, area: str, inference_type: str):
        self.product = product
        self.area = area
        self.inference_type = inference_type
        self.status = "unknown"
        self.start_time = None
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        
        # æ ¹æ“šæ˜¯å¦æœ‰ç•°å¸¸è¨­å®šç‹€æ…‹
        if exc_type is None:
            self.status = "success"
        else:
            self.status = "error"
        
        # è¨˜éŒ„æŒ‡æ¨™
        INFERENCE_LATENCY.labels(
            product=self.product,
            area=self.area,
            inference_type=self.inference_type,
            status=self.status
        ).observe(duration)
        
        INFERENCE_COUNT.labels(
            product=self.product,
            area=self.area,
            inference_type=self.inference_type,
            status=self.status
        ).inc()
```

**ä¿®æ”¹ `core/yolo_inference_model.py`**:
```python
from core.monitoring import InferenceTimer, INFERENCE_COUNT

class YOLOInferenceModel:
    def infer(self, image: np.ndarray, product: str, area: str, ...):
        with InferenceTimer(product, area, "yolo"):
            # ç¾æœ‰æ¨ç†é‚è¼¯
            ...
```

**æä¾› Prometheus Endpoint** (å¦‚æœæœ‰ API):
```python
# app/api.py (æœªä¾†)
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from fastapi import Response

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

**å·¥æ™‚**: 16 å°æ™‚  
**è² è²¬äºº**: DevOps/é–‹ç™¼åœ˜éšŠ

---

#### Task 2.2: çµæ§‹åŒ–æ—¥èªŒ ğŸ“

**ç›®æ¨™**: å¾æ–‡æœ¬æ—¥èªŒè½‰ç‚º JSON æ ¼å¼ï¼Œä¾¿æ–¼è§£æå’Œæœå°‹

**å®‰è£ä¾è³´**:
```bash
pip install python-json-logger
```

**ä¿®æ”¹ `core/logging_config.py`**:
```python
from pythonjsonlogger import jsonlogger

def configure_logging(log_level: str = "INFO"):
    """é…ç½®çµæ§‹åŒ–æ—¥èªŒ."""
    
    # JSON æ ¼å¼åŒ–å™¨
    json_handler = logging.StreamHandler()
    formatter = jsonlogger.JsonFormatter(
        '%(asctime)s %(name)s %(levelname)s %(message)s',
        timestamp=True
    )
    json_handler.setFormatter(formatter)
    
    # æ ¹æ—¥èªŒå™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    root_logger.addHandler(json_handler)
```

**ä½¿ç”¨ç¯„ä¾‹**:
```python
logger.info(
    "Inference completed",
    extra={
        "product": product,
        "area": area,
        "inference_type": inference_type,
        "inference_time_ms": 123.4,
        "result_status": "PASS",
        "detection_count": 5
    }
)

# è¼¸å‡º JSON:
# {
#   "asctime": "2026-01-06T10:30:00.123Z",
#   "name": "core.detection_system",
#   "levelname": "INFO",
#   "message": "Inference completed",
#   "product": "LED",
#   "area": "A",
#   "inference_type": "yolo",
#   "inference_time_ms": 123.4,
#   "result_status": "PASS",
#   "detection_count": 5
# }
```

**å·¥æ™‚**: 8 å°æ™‚  
**è² è²¬äºº**: é–‹ç™¼åœ˜éšŠ

---

#### Task 2.3: å»ºç«‹æ•ˆèƒ½åŸºæº–æ¸¬è©¦ âš¡

**æ–°å¢æ–‡ä»¶**: `tests/test_benchmarks.py`

```python
"""Performance benchmark tests."""
import pytest
import numpy as np
from core.yolo_inference_model import YOLOInferenceModel

@pytest.fixture
def sample_image():
    """Generate a sample test image."""
    return np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)

@pytest.fixture
def yolo_model():
    """Initialize YOLO model for benchmarking."""
    # ä½¿ç”¨æ¸¬è©¦é…ç½®
    ...

def test_inference_latency(benchmark, yolo_model, sample_image):
    """Benchmark YOLO inference latency."""
    result = benchmark(yolo_model.infer, sample_image, "test", "test")
    
    # SLA: æ¨ç†æ‡‰åœ¨ 200ms å…§å®Œæˆï¼ˆCPUï¼‰
    stats = benchmark.stats.stats
    assert stats.mean < 0.2, f"Mean latency {stats.mean:.3f}s exceeds 200ms"

def test_model_load_time(benchmark):
    """Benchmark model loading time."""
    def load_model():
        model = YOLOInferenceModel(config)
        model.initialize("test", "test")
        return model
    
    result = benchmark(load_model)
    
    # SLA: æ¨¡å‹è¼‰å…¥æ‡‰åœ¨ 5s å…§å®Œæˆ
    stats = benchmark.stats.stats
    assert stats.mean < 5.0, f"Model load time {stats.mean:.3f}s exceeds 5s"

@pytest.mark.slow
def test_throughput(yolo_model, sample_image):
    """Test inference throughput."""
    import time
    
    start = time.time()
    count = 100
    
    for _ in range(count):
        yolo_model.infer(sample_image, "test", "test")
    
    duration = time.time() - start
    fps = count / duration
    
    # SLA: æ‡‰é”åˆ° 10 FPS
    assert fps >= 10, f"Throughput {fps:.1f} FPS below target 10 FPS"
```

**åŸ·è¡ŒåŸºæº–æ¸¬è©¦**:
```bash
# åŸ·è¡Œä¸¦ç”Ÿæˆå ±å‘Š
pytest tests/test_benchmarks.py --benchmark-only --benchmark-json=benchmark_results.json

# èˆ‡æ­·å²æ¯”è¼ƒï¼ˆæª¢æ¸¬å›æ­¸ï¼‰
pytest tests/test_benchmarks.py --benchmark-compare=0001 --benchmark-compare-fail=mean:10%
```

**å·¥æ™‚**: 12 å°æ™‚  
**è² è²¬äºº**: QA/é–‹ç™¼åœ˜éšŠ

---

### Phase 2 ç¸½çµ
- **ç¸½å·¥æ™‚**: 36 å°æ™‚ï¼ˆ4.5 å·¥ä½œæ—¥ï¼‰
- **äº¤ä»˜æˆæœ**: å®Œæ•´çš„ç›£æ§é«”ç³»ã€çµæ§‹åŒ–æ—¥èªŒã€æ•ˆèƒ½åŸºæº–
- **é©—æ”¶æ¨™æº–**:
  - âœ… Prometheus æŒ‡æ¨™å¯å°å‡º
  - âœ… JSON æ—¥èªŒå¯è¢« ELK è§£æ
  - âœ… æ•ˆèƒ½åŸºæº–æ¸¬è©¦é€šéä¸¦å»ºç«‹ baseline

---

## ğŸ“… Phase 3: é…ç½®ç³»çµ±é‡æ§‹ï¼ˆç¬¬ 5-8 é€±ï¼‰

### ç›®æ¨™ï¼šä½¿ç”¨ Pydantic V2 é‡æ§‹é…ç½®ï¼Œæå‡é¡å‹å®‰å…¨

#### Task 3.1: é·ç§»åˆ° Pydantic V2

**å®‰è£ Pydantic V2**:
```bash
pip install "pydantic>=2.0"
```

**æ–°å¢æ–‡ä»¶**: `core/config_v2.py`

```python
"""Refactored configuration using Pydantic V2."""
from __future__ import annotations
from pathlib import Path
from typing import Literal, Optional
from pydantic import BaseModel, Field, field_validator, model_validator
from pydantic_settings import BaseSettings

class CameraConfig(BaseModel):
    """Camera configuration."""
    exposure_time: float = Field(gt=0, description="Exposure time in microseconds")
    gain: float = Field(ge=0, le=24, description="Camera gain")
    timeout_ms: int = Field(default=10000, gt=0)
    width: int = Field(default=3072, gt=0)
    height: int = Field(default=2048, gt=0)

class ModelConfig(BaseModel):
    """Model-specific configuration."""
    imgsz: int | list[int] = Field(default=640)
    conf_thres: float = Field(default=0.25, ge=0, le=1)
    iou_thres: float = Field(default=0.45, ge=0, le=1)
    device: str = Field(default="auto")
    
    @field_validator('imgsz')
    @classmethod
    def validate_imgsz(cls, v):
        """Ensure imgsz is valid."""
        if isinstance(v, list):
            if not all(x > 0 and x % 32 == 0 for x in v):
                raise ValueError("imgsz must be multiples of 32")
        elif v <= 0 or v % 32 != 0:
            raise ValueError("imgsz must be multiple of 32")
        return v

class PositionCheckConfig(BaseModel):
    """Position validation configuration."""
    enabled: bool = Field(default=False)
    tolerance: float = Field(default=5.0, ge=0)
    tolerance_unit: Literal["pixel", "percent"] = "percent"
    mode: Literal["bbox", "region", "bbox_region"] = "bbox"
    expected_boxes: dict[str, dict[str, float]] = Field(default_factory=dict)

class DetectionConfigV2(BaseSettings):
    """Main detection configuration with validation."""
    
    # Model paths
    weights: Path = Field(description="Path to YOLO weights")
    
    # Feature flags
    enable_yolo: bool = True
    enable_anomalib: bool = False
    
    # Cache settings
    max_cache_size: int = Field(default=3, ge=1, le=10)
    
    # Output
    output_dir: Path = Field(default=Path("./Result"))
    
    # Camera
    camera: CameraConfig = Field(default_factory=CameraConfig)
    
    # Model defaults
    model: ModelConfig = Field(default_factory=ModelConfig)
    
    # Position check
    position_check: PositionCheckConfig = Field(default_factory=PositionCheckConfig)
    
    @model_validator(mode='after')
    def validate_paths(self):
        """Validate and resolve paths."""
        # è§£æç›¸å°è·¯å¾‘
        if not self.weights.is_absolute():
            self.weights = (Path.cwd() / self.weights).resolve()
        if not self.output_dir.is_absolute():
            self.output_dir = (Path.cwd() / self.output_dir).resolve()
        
        return self
    
    @field_validator('weights')
    @classmethod
    def weights_must_exist(cls, v: Path) -> Path:
        """Validate weights file exists."""
        if not v.exists():
            raise ValueError(f"Weights file not found: {v}")
        return v
    
    class Config:
        env_prefix = "YOLO_"
        env_file = ".env"
        env_file_encoding = "utf-8"

# è¼‰å…¥é…ç½®
def load_config(config_path: str | Path) -> DetectionConfigV2:
    """Load and validate configuration."""
    import yaml
    from core.security import path_validator
    
    safe_path = path_validator.validate_path(config_path, must_exist=True)
    
    with open(safe_path) as f:
        data = yaml.safe_load(f)
    
    # Pydantic V2 è‡ªå‹•é©—è­‰
    return DetectionConfigV2(**data)
```

**å·¥æ™‚**: 24 å°æ™‚  
**è² è²¬äºº**: é–‹ç™¼åœ˜éšŠ

---

#### Task 3.2: é…ç½®åˆä½µè¿½è¹¤

**æ–°å¢**: `core/config_merger.py`

```python
"""Configuration merging with audit trail."""
from typing import Any
import logging

logger = logging.getLogger(__name__)

class ConfigMerger:
    """Merge configurations with tracking."""
    
    def __init__(self):
        self.merge_history: list[dict] = []
    
    def merge(
        self, 
        base: dict[str, Any], 
        override: dict[str, Any], 
        source: str
    ) -> dict[str, Any]:
        """Merge override into base, track changes."""
        result = base.copy()
        changes = []
        
        for key, value in override.items():
            if key in result and result[key] != value:
                changes.append({
                    "key": key,
                    "old": result[key],
                    "new": value,
                    "source": source
                })
            result[key] = value
        
        if changes:
            self.merge_history.append({
                "source": source,
                "changes": changes
            })
            logger.info(
                f"Applied {len(changes)} config overrides from {source}",
                extra={"changes": changes}
            )
        
        return result
    
    def get_audit_trail(self) -> list[dict]:
        """Get configuration merge history."""
        return self.merge_history
```

**å·¥æ™‚**: 8 å°æ™‚

---

### Phase 3 ç¸½çµ
- **ç¸½å·¥æ™‚**: 32 å°æ™‚ï¼ˆ4 å·¥ä½œæ—¥ï¼‰
- **äº¤ä»˜æˆæœ**: é¡å‹å®‰å…¨çš„é…ç½®ç³»çµ±ã€é…ç½®åˆä½µè¿½è¹¤
- **é©—æ”¶æ¨™æº–**:
  - âœ… æ‰€æœ‰é…ç½®æ¬„ä½æœ‰å‹åˆ¥é©—è­‰
  - âœ… é…ç½®éŒ¯èª¤åœ¨è¼‰å…¥æ™‚å³è¢«æ•ç²
  - âœ… é…ç½®åˆä½µéç¨‹å¯è¿½è¹¤

---

## ğŸ“… Phase 4: æ¸¬è©¦å¢å¼·ï¼ˆç¬¬ 9-10 é€±ï¼‰

### ç›®æ¨™ï¼šæå‡æ¸¬è©¦è¦†è“‹ç‡åˆ° 85%+ï¼ŒåŠ å…¥çªè®Šæ¸¬è©¦

#### Task 4.1: æå‡è¦†è“‹ç‡

**ç›®å‰ç‹€æ…‹**: æœªçŸ¥ï¼ˆéœ€å…ˆæ¸¬é‡ï¼‰

**æ¸¬é‡åŸºç·š**:
```bash
pytest --cov=core --cov=app --cov-report=html --cov-report=term-missing
```

**ç›®æ¨™**: 85% è¦†è“‹ç‡

**ç­–ç•¥**:
1. è­˜åˆ¥æœªè¦†è“‹çš„é—œéµè·¯å¾‘
2. ç‚ºéŒ¯èª¤è™•ç†æ·»åŠ æ¸¬è©¦
3. ç‚ºé‚Šç•Œæ¢ä»¶æ·»åŠ æ¸¬è©¦

**ç¯„ä¾‹**:
```python
# tests/test_position_validator_edge_cases.py
def test_position_validator_with_negative_coordinates():
    """Test validator handles negative coordinates."""
    validator = PositionValidator(...)
    detections = [{"cx": -10, "cy": 20, "class": "test"}]
    
    result = validator.validate(detections)
    assert result[0]["position_status"] == "INVALID"

def test_position_validator_with_out_of_bounds():
    """Test validator handles out-of-bounds coordinates."""
    ...
```

**å·¥æ™‚**: 16 å°æ™‚  
**è² è²¬äºº**: QA/é–‹ç™¼åœ˜éšŠ

---

#### Task 4.2: çªè®Šæ¸¬è©¦ï¼ˆMutation Testingï¼‰

**å®‰è£**:
```bash
pip install mutmut
```

**åŸ·è¡Œçªè®Šæ¸¬è©¦**:
```bash
# å° core/position_validator.py é€²è¡Œçªè®Šæ¸¬è©¦
mutmut run --paths-to-mutate=core/position_validator.py

# æŸ¥çœ‹çµæœ
mutmut results

# æŸ¥çœ‹å€–å­˜çš„çªè®Šï¼ˆæ¸¬è©¦æ‡‰è©²æ•ç²ä½†æ²’æœ‰ï¼‰
mutmut show
```

**ç›®æ¨™**: çªè®Šåˆ†æ•¸ > 80%

**å·¥æ™‚**: 8 å°æ™‚  
**è² è²¬äºº**: QAåœ˜éšŠ

---

#### Task 4.3: å¥‘ç´„æ¸¬è©¦ï¼ˆContract Testingï¼‰

**é‡å°æ¨¡å‹è¼¸å‡ºæ ¼å¼**:

```python
# tests/test_contracts.py
from pydantic import BaseModel, Field

class YOLODetection(BaseModel):
    """Contract for YOLO detection output."""
    class_name: str = Field(alias="class")
    confidence: float = Field(ge=0, le=1)
    bbox: list[float] = Field(min_length=4, max_length=4)
    cx: float
    cy: float

class YOLOInferenceResult(BaseModel):
    """Contract for YOLO inference result."""
    status: str
    detections: list[YOLODetection]
    inference_time_ms: float = Field(gt=0)
    missing_items: list[str]

def test_yolo_inference_output_contract():
    """Ensure YOLO output matches contract."""
    result = run_inference(test_image)
    
    # Pydantic è‡ªå‹•é©—è­‰å¥‘ç´„
    validated = YOLOInferenceResult(**result)
    
    assert validated.status in {"PASS", "FAIL", "ERROR"}
```

**å·¥æ™‚**: 12 å°æ™‚

---

### Phase 4 ç¸½çµ
- **ç¸½å·¥æ™‚**: 36 å°æ™‚ï¼ˆ4.5 å·¥ä½œæ—¥ï¼‰
- **äº¤ä»˜æˆæœ**: 85%+ è¦†è“‹ç‡ã€çªè®Šæ¸¬è©¦ã€å¥‘ç´„æ¸¬è©¦
- **é©—æ”¶æ¨™æº–**:
  - âœ… æ¸¬è©¦è¦†è“‹ç‡ >= 85%
  - âœ… çªè®Šåˆ†æ•¸ > 80%
  - âœ… æ‰€æœ‰é—œéµå¥‘ç´„æœ‰æ¸¬è©¦ä¿è­·

---

## ğŸ“… Phase 5: API é–‹ç™¼ï¼ˆç¬¬ 11-16 é€±ï¼‰

### ç›®æ¨™ï¼šæä¾› REST APIï¼Œæ”¯æ´éåŒæ­¥æ¨ç†

#### Task 5.1: FastAPI åŸºç¤æ¶æ§‹

**å®‰è£ä¾è³´**:
```bash
pip install fastapi uvicorn[standard] python-multipart
```

**æ–°å¢æ–‡ä»¶**: `api/main.py`

```python
"""FastAPI application for YOLO inference."""
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import numpy as np
import cv2
from core.detection_system import DetectionSystem

app = FastAPI(
    title="YOLO11 Inference API",
    description="Industrial vision inspection system",
    version="1.0.0"
)

# åˆå§‹åŒ–æª¢æ¸¬ç³»çµ±ï¼ˆå•Ÿå‹•æ™‚ï¼‰
detection_system = DetectionSystem()

class InferenceRequest(BaseModel):
    """Inference request model."""
    product: str
    area: str
    inference_type: str = "yolo"

class InferenceResponse(BaseModel):
    """Inference response model."""
    status: str
    detections: list[dict]
    missing_items: list[str]
    inference_time_ms: float

@app.post("/api/v1/infer", response_model=InferenceResponse)
async def infer(
    image: UploadFile = File(...),
    product: str = Form(...),
    area: str = Form(...),
    inference_type: str = Form("yolo")
):
    """Run inference on uploaded image."""
    # è®€å–å½±åƒ
    contents = await image.read()
    nparr = np.frombuffer(contents, np.uint8)
    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    if frame is None:
        raise HTTPException(400, "Invalid image format")
    
    # åŸ·è¡Œæ¨ç†
    result = detection_system.detect(product, area, inference_type, frame=frame)
    
    return InferenceResponse(**result)

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy"}

@app.on_event("shutdown")
def shutdown_event():
    """Cleanup on shutdown."""
    detection_system.shutdown()
```

**å•Ÿå‹•æœå‹™**:
```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

**å·¥æ™‚**: 24 å°æ™‚  
**è² è²¬äºº**: å¾Œç«¯é–‹ç™¼åœ˜éšŠ

---

#### Task 5.2: éåŒæ­¥æ¨ç†éšŠåˆ—

**å•é¡Œ**: FastAPI æ˜¯ asyncï¼Œä½† YOLO æ¨ç†æ˜¯ CPU å¯†é›†å‹åŒæ­¥æ“ä½œ

**è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨ ThreadPoolExecutor

```python
from fastapi import BackgroundTasks
from concurrent.futures import ThreadPoolExecutor
import asyncio

# å…¨å±€åŸ·è¡Œå™¨
executor = ThreadPoolExecutor(max_workers=4)

async def run_inference_async(
    frame: np.ndarray,
    product: str,
    area: str,
    inference_type: str
) -> dict:
    """Run inference in thread pool."""
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        executor,
        detection_system.detect,
        product,
        area,
        inference_type,
        frame
    )
    return result

@app.post("/api/v1/infer/async")
async def infer_async(image: UploadFile, ...):
    """Async inference endpoint."""
    frame = await load_image(image)
    result = await run_inference_async(frame, product, area, inference_type)
    return result
```

**å·¥æ™‚**: 16 å°æ™‚

---

#### Task 5.3: OpenAPI æ–‡æª”å’Œå®¢æˆ¶ç«¯

**è‡ªå‹•ç”Ÿæˆ OpenAPI spec**:
```bash
# FastAPI è‡ªå‹•æä¾›
curl http://localhost:8000/openapi.json > openapi.json
```

**ç”Ÿæˆ Python å®¢æˆ¶ç«¯**:
```bash
pip install openapi-python-client
openapi-python-client generate --url http://localhost:8000/openapi.json
```

**ç”Ÿæˆ TypeScript å®¢æˆ¶ç«¯**:
```bash
npm install -g @openapitools/openapi-generator-cli
openapi-generator-cli generate -i openapi.json -g typescript-axios -o ./client-ts
```

**å·¥æ™‚**: 8 å°æ™‚

---

### Phase 5 ç¸½çµ
- **ç¸½å·¥æ™‚**: 48 å°æ™‚ï¼ˆ6 å·¥ä½œæ—¥ï¼‰
- **äº¤ä»˜æˆæœ**: å®Œæ•´çš„ REST APIã€éåŒæ­¥æ¨ç†ã€å®¢æˆ¶ç«¯
- **é©—æ”¶æ¨™æº–**:
  - âœ… API å¯è™•ç†ä¸¦ç™¼è«‹æ±‚
  - âœ… OpenAPI æ–‡æª”å®Œæ•´
  - âœ… å®¢æˆ¶ç«¯å¯æ­£å¸¸èª¿ç”¨

---

## ğŸ“… Phase 6: å®¹å™¨åŒ–èˆ‡éƒ¨ç½²ï¼ˆç¬¬ 17-18 é€±ï¼‰

### Task 6.1: Docker åŒ–

**æ–°å¢æª”æ¡ˆ**: `Dockerfile`

```dockerfile
FROM pytorch/pytorch:2.4.1-cuda12.1-cudnn8-runtime

WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    libgl1 \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# è¤‡è£½ä¾è³´æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ä»£ç¢¼
COPY . .

# å®‰è£æ‡‰ç”¨
RUN pip install --no-cache-dir -e .

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

EXPOSE 8000

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  yolo-inference:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models:ro
      - ./Result:/app/Result
    environment:
      - YOLO_LOG_LEVEL=INFO
      - YOLO_MAX_CACHE_SIZE=3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
  
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

**å·¥æ™‚**: 16 å°æ™‚  
**è² è²¬äºº**: DevOpsåœ˜éšŠ

---

## ğŸ“Š ç¸½é«”æ™‚é–“è¦åŠƒ

| Phase | å·¥ä½œé€± | å·¥æ™‚ | é—œéµäº¤ä»˜ |
|-------|--------|------|----------|
| Phase 1: ç·Šæ€¥ä¿®å¾© | ç¬¬ 1 é€± | 14h | CI ç©©å®šã€ä¾è³´ä¿®å¾©ã€å®‰å…¨åŠ å›º |
| Phase 2: å¯è§€æ¸¬æ€§ | ç¬¬ 2-4 é€± | 36h | ç›£æ§ã€æ—¥èªŒã€åŸºæº–æ¸¬è©¦ |
| Phase 3: é…ç½®é‡æ§‹ | ç¬¬ 5-8 é€± | 32h | Pydantic V2ã€é…ç½®è¿½è¹¤ |
| Phase 4: æ¸¬è©¦å¢å¼· | ç¬¬ 9-10 é€± | 36h | 85% è¦†è“‹ç‡ã€çªè®Šæ¸¬è©¦ |
| Phase 5: API é–‹ç™¼ | ç¬¬ 11-16 é€± | 48h | REST APIã€éåŒæ­¥æ¨ç† |
| Phase 6: å®¹å™¨åŒ– | ç¬¬ 17-18 é€± | 16h | Dockerã€K8s ready |

**ç¸½è¨ˆ**: ~182 å°æ™‚ï¼ˆç´„ 23 å·¥ä½œæ—¥ï¼Œå³ 4.5 å€‹æœˆä»¥æ¯é€± 10 å°æ™‚è¨ˆç®—ï¼‰

---

## âœ… é©—æ”¶æ¨™æº–ç¸½è¦½

### æŠ€è¡“æŒ‡æ¨™
- âœ… CI/CD: æ‰€æœ‰æª¢æŸ¥å…¨ç¶ ï¼Œç„¡ `continue-on-error`
- âœ… æ¸¬è©¦è¦†è“‹ç‡: >= 85%
- âœ… çªè®Šåˆ†æ•¸: > 80%
- âœ… API éŸ¿æ‡‰æ™‚é–“: P95 < 200msï¼ˆCPUï¼‰/ P95 < 50msï¼ˆGPUï¼‰
- âœ… å®¹å™¨å•Ÿå‹•æ™‚é–“: < 30 ç§’

### å®‰å…¨æŒ‡æ¨™
- âœ… ç„¡é«˜å±æ¼æ´ï¼ˆ`safety check` å…¨éï¼‰
- âœ… æ‰€æœ‰è·¯å¾‘é©—è­‰é€šé
- âœ… æ‰€æœ‰ YAML ä½¿ç”¨ `safe_load`

### å¯é æ€§æŒ‡æ¨™
- âœ… éŒ¯èª¤ç‡ < 0.1%ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
- âœ… æ¨ç† SLA é”æˆç‡ > 99%
- âœ… æ¨¡å‹è¼‰å…¥å¤±æ•—è‡ªå‹•é‡è©¦æˆåŠŸ

---

## ğŸ“ åœ˜éšŠåŸ¹è¨“éœ€æ±‚

### å»ºè­°åŸ¹è¨“ä¸»é¡Œ
1. **Pydantic V2 æ·±åº¦æ‡‰ç”¨** (4h)
2. **FastAPI éåŒæ­¥æœ€ä½³å¯¦è¸** (8h)
3. **Prometheus + Grafana ç›£æ§** (4h)
4. **Docker èˆ‡ Kubernetes åŸºç¤** (8h)
5. **å¥‘ç´„æ¸¬è©¦èˆ‡ API è¨­è¨ˆ** (4h)

---

## ğŸ“ˆ æˆåŠŸæŒ‡æ¨™

å®Œæˆæ­¤è·¯ç·šåœ–å¾Œï¼Œå°ˆæ¡ˆæ‡‰é”åˆ°ï¼š

| ç¶­åº¦ | ç›®å‰ | ç›®æ¨™ |
|------|------|------|
| æ•´é«”è©•åˆ† | 7.5/10 | 9.0/10 |
| æ¸¬è©¦è¦†è“‹ç‡ | ~60% | 85%+ |
| CI ç©©å®šæ€§ | æœ‰ warnings | å…¨ç¶  |
| éƒ¨ç½²è¤‡é›œåº¦ | æ‰‹å‹• | å®¹å™¨åŒ–è‡ªå‹• |
| å¯è§€æ¸¬æ€§ | åŸºç¤æ—¥èªŒ | å®Œæ•´ç›£æ§ |
| API å¯ç”¨æ€§ | ç„¡ | REST + async |

---

**æ–‡æª”ç‰ˆæœ¬**: 1.0  
**æœ€å¾Œæ›´æ–°**: 2026-01-06  
**ä¸‹æ¬¡å¯©æŸ¥**: 2026-04-06
